<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/FastAPI-0.109+-00d9ff?style=for-the-badge&logo=fastapi&logoColor=white" alt="FastAPI">
  <img src="https://img.shields.io/badge/PyTorch-2.1+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white" alt="PyTorch">
  <img src="https://img.shields.io/badge/CUDA-11.8+-76B900?style=for-the-badge&logo=nvidia&logoColor=white" alt="CUDA">
  <img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge" alt="License">
</p>

<h1 align="center">ğŸ¬ Text-to-Video Generator</h1>

<p align="center">
  <strong>AI-powered text-to-video generation with neural lip-sync capabilities</strong>
</p>

<p align="center">
  Transform text prompts into professional talking-head videos with accurate lip synchronization.
  <br>
  Powered by CogVideoX, XTTS-v2, Wav2Lip, and Real-ESRGAN.
</p>

<p align="center">
  <a href="#features">Features</a> â€¢
  <a href="#demo">Demo</a> â€¢
  <a href="#quick-start">Quick Start</a> â€¢
  <a href="#api-reference">API</a> â€¢
  <a href="#deployment">Deployment</a>
</p>

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ¬ **Text-to-Video** | Generate video from text using CogVideoX diffusion models |
| ğŸ¤ **Neural TTS** | High-quality speech synthesis with XTTS-v2 (17 languages) |
| ğŸ‘„ **Lip Sync** | Accurate lip synchronization using Wav2Lip GAN |
| ğŸ“º **HD Upscaling** | 4x video enhancement with Real-ESRGAN |
| ğŸ­ **Voice Cloning** | Clone any voice from a 6-second audio sample |
| ğŸŒ **REST API** | Production-ready FastAPI backend |
| ğŸ’» **Modern UI** | Beautiful web interface with real-time progress |
| ğŸ³ **Docker Ready** | One-command deployment with docker-compose |

## ğŸ–¥ï¸ Demo

> **Live Demo**: [text-to-video-generator.vercel.app](https://text-to-video-generator.vercel.app)

<p align="center">
  <em>Enter your text prompt and watch AI generate a lip-synced video!</em>
</p>

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Text Prompt                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CogVideoX       â”‚                     â”‚   XTTS-v2         â”‚
â”‚   Video Gen       â”‚                     â”‚   Speech Gen      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                           â”‚
        â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚     Wav2Lip         â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚     Lip Sync        â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Real-ESRGAN       â”‚ (Optional)
                  â”‚   Upscaling         â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Final MP4         â”‚
                  â”‚   Download          â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10-3.11
- NVIDIA GPU with 12GB+ VRAM (RTX 3060 or better)
- CUDA 11.8+
- FFmpeg

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/text-to-video-generator.git
cd text-to-video-generator

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download model checkpoints
python scripts/download_models.py

# Copy environment config
cp .env.example .env

# Start the server
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```

Open http://localhost:8000 in your browser.

### Docker Deployment

```bash
# Build and run with Docker Compose
docker-compose up -d

# Check logs
docker-compose logs -f
```

## ğŸ“– API Reference

### Generate Video

```bash
POST /api/generate
```

**Request Body:**

```json
{
  "prompt": "Hello! Welcome to our AI demonstration.",
  "duration": 6,
  "language": "en",
  "upscale": true
}
```

**Response:**

```json
{
  "job_id": "job_20260121_123456_abc12345",
  "status": "pending",
  "message": "Video generation started"
}
```

### Check Status

```bash
GET /api/status/{job_id}
```

**Response:**

```json
{
  "job": {
    "job_id": "job_20260121_123456_abc12345",
    "status": "generating_video",
    "progress": 35.5,
    "current_step": "Generating video frames..."
  }
}
```

### Download Video

```bash
GET /api/download/{job_id}
```

Returns the generated MP4 video file.

### Health Check

```bash
GET /health
```

**Response:**

```json
{
  "status": "healthy",
  "version": "1.0.0",
  "gpu_available": true,
  "gpu_name": "NVIDIA RTX 4090",
  "gpu_memory_gb": 24.0
}
```

## ğŸŒ Supported Languages

| Code | Language | Code | Language |
|------|----------|------|----------|
| `en` | English | `ru` | Russian |
| `es` | Spanish | `nl` | Dutch |
| `fr` | French | `cs` | Czech |
| `de` | German | `ar` | Arabic |
| `it` | Italian | `zh-cn` | Chinese |
| `pt` | Portuguese | `ko` | Korean |
| `pl` | Polish | `ja` | Japanese |
| `tr` | Turkish | `hi` | Hindi |

## ğŸ“ Project Structure

```
text-to-video-generator/
â”œâ”€â”€ api/                    # FastAPI backend
â”‚   â”œâ”€â”€ main.py             # Application entry point
â”‚   â”œâ”€â”€ routes/             # API endpoints
â”‚   â””â”€â”€ models/             # Pydantic schemas
â”œâ”€â”€ core/                   # Pipeline orchestration
â”‚   â”œâ”€â”€ config.py           # Configuration management
â”‚   â”œâ”€â”€ pipeline.py         # Main workflow
â”‚   â””â”€â”€ utils.py            # Utility functions
â”œâ”€â”€ modules/                # AI model wrappers
â”‚   â”œâ”€â”€ video_generator/    # CogVideoX integration
â”‚   â”œâ”€â”€ tts/                # XTTS-v2 integration
â”‚   â”œâ”€â”€ lip_sync/           # Wav2Lip integration
â”‚   â””â”€â”€ upscaler/           # Real-ESRGAN integration
â”œâ”€â”€ frontend/               # Web interface
â”‚   â”œâ”€â”€ index.html          # Main HTML
â”‚   â”œâ”€â”€ styles/             # CSS styles
â”‚   â””â”€â”€ scripts/            # JavaScript
â”œâ”€â”€ scripts/                # Utility scripts
â”‚   â””â”€â”€ download_models.py  # Model downloader
â”œâ”€â”€ outputs/                # Generated videos
â”œâ”€â”€ checkpoints/            # Model weights
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ Dockerfile              # Container definition
â””â”€â”€ docker-compose.yml      # Docker orchestration
```

## âš™ï¸ Configuration

Configuration is managed through environment variables. Copy `.env.example` to `.env` and customize:

| Variable | Default | Description |
|----------|---------|-------------|
| `APP_API_PORT` | `8000` | API server port |
| `APP_TORCH_DTYPE` | `float16` | Model precision |
| `VIDEO_MODEL_ID` | `THUDM/CogVideoX-2b` | Video model |
| `VIDEO_HEIGHT` | `480` | Output height |
| `VIDEO_WIDTH` | `720` | Output width |
| `TTS_LANGUAGE` | `en` | Default language |
| `UPSCALE_SCALE` | `4` | Upscale factor |

## ğŸ® GPU Requirements

| Configuration | VRAM Required | Recommended GPU |
|---------------|---------------|-----------------|
| Minimum | 12GB | RTX 3060 |
| Standard | 16GB | RTX 4070 |
| Optimal | 24GB+ | RTX 4090, A100 |

The pipeline automatically manages GPU memory by loading/unloading modules sequentially.

## ğŸ¤ Contributing

Contributions are welcome! Please read our [Contributing Guidelines](CONTRIBUTING.md) first.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [CogVideoX](https://github.com/THUDM/CogVideo) - Video generation model
- [Coqui TTS](https://github.com/coqui-ai/TTS) - Text-to-speech
- [Wav2Lip](https://github.com/Rudrabha/Wav2Lip) - Lip synchronization
- [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN) - Video upscaling

---

<p align="center">
  Made with â¤ï¸ by AI-powered development
</p>
